{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pathlib\n",
    "import ase.io\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nequip.train import Trainer\n",
    "from nequip.utils import Config\n",
    "from nequip.data import AtomicData, Collater, dataset_from_config\n",
    "from nequip.data import AtomicDataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train_dir = \"results/nmr_prod_all/bmrb_mace_prod_all1/\"\n",
    "args_dataset_config = os.path.join(\"configs\", \"nmr\", \"bmrb_prod_all_test.yaml\")\n",
    "\n",
    "model_name = \"best_model.pth\"\n",
    "args_model = os.path.join(args_train_dir, model_name)\n",
    "device = \"cuda:2\"\n",
    "\n",
    "test_frame_index = 206\n",
    "minimization_max_steps = 1000\n",
    "minimization_threshold_error = 1.\n",
    "dtau = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a training session model\n",
    "model, model_config = Trainer.load_model_from_training_session(\n",
    "    traindir=args_train_dir, model_name=model_name\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_config = Config.from_file(str(args_dataset_config), defaults={})\n",
    "model_config.update(test_config)\n",
    "\n",
    "dataset, _ = dataset_from_config(model_config, prefix=\"test_dataset\")\n",
    "pdb_code = dataset.datasets[test_frame_index].file_name.split('/')[-1].split('.')[0]\n",
    "c = Collater.for_dataset(dataset, exclude_keys=[])\n",
    "\n",
    "test_idcs = torch.arange(len(dataset.datasets))\n",
    "\n",
    "this_batch_test_indexes = test_idcs[test_frame_index : test_frame_index + 1]\n",
    "datas = [dataset[int(idex)] for idex in this_batch_test_indexes]\n",
    "\n",
    "batch = c.collate(datas)\n",
    "batch = batch.to(device)\n",
    "input_ = AtomicData.to_AtomicDataDict(batch)\n",
    "\n",
    "if AtomicDataDict.PER_ATOM_ENERGY_KEY in input_:\n",
    "    not_nan_edge_filter = torch.isin(input_[AtomicDataDict.EDGE_INDEX_KEY][0], torch.argwhere(~torch.isnan(input_[AtomicDataDict.PER_ATOM_ENERGY_KEY].flatten())).flatten())\n",
    "    input_[AtomicDataDict.EDGE_INDEX_KEY] = input_[AtomicDataDict.EDGE_INDEX_KEY][:, not_nan_edge_filter]\n",
    "    input_[AtomicDataDict.EDGE_CELL_SHIFT_KEY] = input_[AtomicDataDict.EDGE_CELL_SHIFT_KEY][not_nan_edge_filter]\n",
    "    input_[AtomicDataDict.ORIG_BATCH_KEY] = input_[AtomicDataDict.BATCH_KEY].clone()\n",
    "    input_[AtomicDataDict.BATCH_KEY] = input_[AtomicDataDict.BATCH_KEY][~torch.isnan(input_[AtomicDataDict.PER_ATOM_ENERGY_KEY]).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"pos\": [],\n",
    "    \"cs\": [],\n",
    "    \"loss\": [],\n",
    "}\n",
    "\n",
    "with tqdm(total=minimization_max_steps) as pbar:\n",
    "    for i in range(minimization_max_steps):\n",
    "        input_[AtomicDataDict.POSITIONS_KEY].requires_grad_(True)\n",
    "        out_ = model(input_)\n",
    "\n",
    "        pred_cs = out_[AtomicDataDict.PER_ATOM_ENERGY_KEY]\n",
    "        target_cs = input_[AtomicDataDict.PER_ATOM_ENERGY_KEY]\n",
    "        not_nan_node_filter = torch.argwhere(~torch.isnan(input_[AtomicDataDict.PER_ATOM_ENERGY_KEY].flatten())).flatten()\n",
    "\n",
    "        loss = torch.pow((pred_cs[not_nan_node_filter] - target_cs[not_nan_node_filter]), 2)\n",
    "\n",
    "        forces = -torch.autograd.grad(\n",
    "            [loss.sum()],\n",
    "            [out_[AtomicDataDict.POSITIONS_KEY]],\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        out_[AtomicDataDict.POSITIONS_KEY].requires_grad_(False)\n",
    "        input_[AtomicDataDict.POSITIONS_KEY] += forces.detach() * dtau\n",
    "\n",
    "        results['pos'].append(input_[AtomicDataDict.POSITIONS_KEY].detach().cpu().numpy())\n",
    "        results['cs'].append(pred_cs.detach().cpu().numpy().flatten())\n",
    "        results['loss'].append(loss.detach().sum().cpu().numpy())\n",
    "\n",
    "        pbar.update(1)\n",
    "        if loss.sum().item() < minimization_threshold_error:\n",
    "            pbar.update(minimization_max_steps - i - 1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_xyz_filename(args_train_dir, pdb_code):\n",
    "    for item in pathlib.Path(args_train_dir).rglob(\"*evaluation.xyz\"):\n",
    "        if pdb_code in item.name:\n",
    "            return str(item)\n",
    "    raise Exception\n",
    "\n",
    "test_xyz_filename = get_test_xyz_filename(args_train_dir, pdb_code)\n",
    "test_minimized_xyz_filename = test_xyz_filename.split('.')[0] + \"_minimized.xyz\"\n",
    "\n",
    "test_xyz = ase.io.read(test_xyz_filename, index=\":\", format=\"extxyz\")[0].copy()\n",
    "test_xyz.arrays['positions'] = results['pos'][-1]\n",
    "test_xyz.arrays['energies'] = results['cs'][-1]\n",
    "\n",
    "ase.io.write(\n",
    "    test_minimized_xyz_filename,\n",
    "    test_xyz,\n",
    "    format=\"extxyz\",\n",
    "    append=False,\n",
    ")\n",
    "print(f\"minimization file {test_minimized_xyz_filename} saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fesnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
